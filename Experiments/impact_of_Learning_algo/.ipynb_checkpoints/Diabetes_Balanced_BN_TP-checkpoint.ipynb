{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/catarina-moreira/causabilityXAi/blob/master/Diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJKi9vUWhXI2"
   },
   "source": [
    "# Demystifying Predictive Black-Box Models: An Interpretable Probabilistic Approach\n",
    "\n",
    "Catarina Moreira, Yu-Liang Chou, Mythreyi Velmurugan, Renuka Sindhgatta Rajan, Chun Ouyang, Peter Bruza\n",
    "\n",
    "**Abstract** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:26.591355Z",
     "start_time": "2020-07-06T06:44:26.588054Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5Arnv2vDgvSv"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow\n",
    "try:\n",
    "    # tensorflow_version only exists in Colab\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:27.087606Z",
     "start_time": "2020-07-06T06:44:27.085715Z"
    }
   },
   "outputs": [],
   "source": [
    "# library to deal with Bayesian Networks\n",
    "#!pip install pyagrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:27.948544Z",
     "start_time": "2020-07-06T06:44:27.417561Z"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:30.461848Z",
     "start_time": "2020-07-06T06:44:28.019436Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ypF1SQdQh8vx"
   },
   "outputs": [],
   "source": [
    "# for reproduciability reasons:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import auxiliary functions\n",
    "from learning import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:30.702708Z",
     "start_time": "2020-07-06T06:44:30.701038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "bisN9ZyOlN0w",
    "outputId": "7e09ce1e-d4d3-48db-de97-cf214a70a5ef"
   },
   "outputs": [],
   "source": [
    "# use only if opening on google colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "**Context**\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "**Content**\n",
    "The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the \n",
    "- number of pregnancies the patient has had, \n",
    "- their BMI, \n",
    "- insulin level, \n",
    "- age,\n",
    "- glucose,\n",
    "- blood pressure,\n",
    "- skin thickness,\n",
    "- Diabetes pedigree function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6F3cADNh_xD"
   },
   "source": [
    "### Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:30.931371Z",
     "start_time": "2020-07-06T06:44:30.929597Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OUX7Tyc6v023"
   },
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = \"/Users/catarina/Google Drive/DDS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:31.171925Z",
     "start_time": "2020-07-06T06:44:31.166434Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "qnF5zKygiH9x",
    "outputId": "a1458040-afec-4812-982c-d95789824fa2"
   },
   "outputs": [],
   "source": [
    "# name of dataset\n",
    "DATASET_NAME = \"diabetes.csv\"\n",
    "\n",
    "# variable containing the class labels in this case the dataset contains:\n",
    "# 0 - if not diabetes\n",
    "# 1 - if diabetes\n",
    "class_var = \"Outcome\"\n",
    "\n",
    "# load dataset\n",
    "dataset_path = PATH + \"datasets/\" + DATASET_NAME\n",
    "data = pd.read_csv( dataset_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:31.491883Z",
     "start_time": "2020-07-06T06:44:31.480452Z"
    }
   },
   "outputs": [],
   "source": [
    "# features\n",
    "feature_names = data.drop([class_var], axis=1).columns.to_list()\n",
    "\n",
    "# check how balanced the classes are\n",
    "data.groupby(class_var).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:31.819223Z",
     "start_time": "2020-07-06T06:44:31.804987Z"
    }
   },
   "outputs": [],
   "source": [
    "# balance dataset\n",
    "sampled_data = data.sample(frac=1)\n",
    "sampled_data = sampled_data[ sampled_data[\"Outcome\"] == 0]\n",
    "no_data = sampled_data.sample(frac=1)[0:268]\n",
    "\n",
    "yes_data = data[ data[\"Outcome\"] == 1]\n",
    "\n",
    "balanced_data = [no_data,yes_data]\n",
    "balanced_data = pd.concat(balanced_data)\n",
    "\n",
    "# check how balanced the classes are\n",
    "balanced_data.groupby(class_var).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Model for the Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:32.479520Z",
     "start_time": "2020-07-06T06:44:32.464059Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply one hot encoder to data\n",
    "# standardize the input between 0 and 1\n",
    "X, Y, encoder, scaler = encode_data( balanced_data, class_var)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(balanced_data[class_var].unique())\n",
    " \n",
    "flag = False  # DO NOT CHANGE! Data has already been generated. \n",
    "if flag:\n",
    "    # save training, test and validation data\n",
    "    generate_save_training_data( dataset_path, X, Y)\n",
    "    \n",
    "else:\n",
    "    # load existing training data\n",
    "    X_train, Y_train, X_test, Y_test, X_validation, Y_validation= load_training_data( dataset_path )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:33.444442Z",
     "start_time": "2020-07-06T06:44:33.441588Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate models for grid search\n",
    "if flag:\n",
    "    models = grid_search_model_generator( n_features, n_classes )\n",
    "\n",
    "    # perform grid_search\n",
    "    HISTORY_DICT = perform_grid_search( models, PATH, DATASET_NAME.replace(\".csv\",\"\"), \n",
    "                                   X_train, Y_train, \n",
    "                                   X_validation, Y_validation, X_test, Y_test, \n",
    "                                   batch_size=8, epochs=150 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:34.437226Z",
     "start_time": "2020-07-06T06:44:34.365132Z"
    }
   },
   "outputs": [],
   "source": [
    "path_serialisation_model = PATH + \"training/\" + DATASET_NAME.replace(\".csv\", \"\") + \"/model/\" \n",
    "path_serialisation_histr = PATH + \"training/\" + DATASET_NAME.replace(\".csv\", \"\") + \"/history/\" \n",
    "\n",
    "# the best performing model was obtained with 5 hidden layers with 12 neurons each\n",
    "model_name = \"model_h5_N12\"\n",
    "    \n",
    "if flag:\n",
    "    \n",
    "    # get respective model training history and model\n",
    "    model_history = HISTORY_DICT[ model_name ][0]\n",
    "    model = HISTORY_DICT[ model_name ][1]\n",
    "\n",
    "    # save model and model history to file\n",
    "    save_model_history(  model_history, model_name, path_serialisation_histr )\n",
    "    save_model( model, model_name, path_serialisation_model )\n",
    "else:\n",
    "    model_history = load_model_history( model_name, path_serialisation_histr )\n",
    "    model = load_model( model_name, path_serialisation_model )\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:35.658999Z",
     "start_time": "2020-07-06T06:44:35.488356Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate loaded model on test and training data\n",
    "optim = keras.optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "train_loss, train_acc = model.evaluate(X_train, Y_train, verbose=1)\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "print('\\n[Accuracy] Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "print('[Loss] Train: %.3f, Test: %.3f' % (train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:36.635423Z",
     "start_time": "2020-07-06T06:44:36.633412Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_model_history( model_history, 'Accuracy' )\n",
    "#plot_model_history( model_history, 'Loss' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:37.537287Z",
     "start_time": "2020-07-06T06:44:37.535644Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "#plot_ROC_Curve( model, X_test, Y_test, n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific datapoints for local evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:44:38.491115Z",
     "start_time": "2020-07-06T06:44:38.428193Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "local_data_dict = generate_local_predictions( X_test, Y_test, model, scaler, encoder )\n",
    "\n",
    "# wrapping up information\n",
    "true_positives = []\n",
    "true_negatives = []\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "for instance in local_data_dict:\n",
    "    \n",
    "    if( instance['prediction_type'] == 'TRUE POSITIVE'):\n",
    "        true_positives.append(instance)\n",
    "\n",
    "    if( instance['prediction_type'] == 'TRUE NEGATIVE' ):\n",
    "        true_negatives.append(instance)\n",
    "        \n",
    "    if( instance['prediction_type'] == 'FALSE POSITIVE' ):\n",
    "        false_positives.append(instance)\n",
    "        \n",
    "    if( instance['prediction_type'] == 'FALSE NEGATIVE' ):\n",
    "        false_negatives.append(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Explanations with Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRUE POSITIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:45:33.859456Z",
     "start_time": "2020-07-06T06:45:21.541017Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_lst = [\"No\", \"Yes\"]\n",
    "class_var = \"Diabetes?\"\n",
    "\n",
    "for instance in true_positives[0:5]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:46:41.373032Z",
     "start_time": "2020-07-06T06:46:20.929056Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[5:10]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:47:10.031293Z",
     "start_time": "2020-07-06T06:46:57.629099Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[10:15]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:47:46.822653Z",
     "start_time": "2020-07-06T06:47:34.396222Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[15:20]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:48:31.368169Z",
     "start_time": "2020-07-06T06:48:19.187443Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[20:25]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:49:51.462869Z",
     "start_time": "2020-07-06T06:49:25.755216Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[25:30]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:50:24.234430Z",
     "start_time": "2020-07-06T06:50:17.140811Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for instance in true_positives[30:]:\n",
    "\n",
    "    # get instance index\n",
    "    indx = instance['index']\n",
    "    print(\"INDEX = \" + str(indx))\n",
    "    \n",
    "    [bn, inference, infoBN, markov] = generate_BN_explanationsMB(instance, label_lst, feature_names, \n",
    "                                                        class_var, encoder, scaler, model, PATH, DATASET_NAME, \n",
    "                                                                 variance = 0.25)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMi9CxWI4QNMgqjhwSe/hVD",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Diabetes.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
